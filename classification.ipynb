{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.optimizers import Adam\n",
    "import string\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "stop_words = stopwords.words('indonesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56360/893036123.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_train = df_train.append(df_test)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('datasets/new_dataset.csv')\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "selected_column = df_train.columns[1:]\n",
    "df_test = pd.read_csv('datasets/data_test.csv')\n",
    "total_train_data = len(df_train)\n",
    "df_train = df_train.append(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingText(text: str):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    text = text.lower()\n",
    "    text = stemmer.stem(text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text_tokenize = word_tokenize(text)\n",
    "    text = [t for t in text_tokenize if t not in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['questions'] = df_train['questions'].apply(lambda x: preprocessingText(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 1000\n",
    "max_words = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train['questions'].values\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(max_words)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_train = tokenizer.texts_to_sequences(X)\n",
    "X_train = tf.keras.utils.pad_sequences(X_train, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_data = X_train[0:total_train_data]\n",
    "X_test_data = X_train[total_train_data:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df_train.columns\n",
    "y_train = np.array(df_train[columns[1:]])\n",
    "y_train_data = y_train[0:total_train_data]\n",
    "y_test_data = y_train[total_train_data:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[158], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m         values \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39mrstrip()\u001b[39m.\u001b[39mrsplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m         word \u001b[39m=\u001b[39m values[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m         embed \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(values[\u001b[39m1\u001b[39;49m:], dtype\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m         embeddings_index[word] \u001b[39m=\u001b[39m embed\n\u001b[1;32m      9\u001b[0m embed_size \u001b[39m=\u001b[39m \u001b[39m300\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('w2vec_wiki_id300_0.txt', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        embed = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embed\n",
    "\n",
    "embed_size = 300\n",
    "word_index = tokenizer.word_index\n",
    "embed_num_words = min(max_words, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embed_size), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word_index.items():\n",
    "    if i >= max_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3723 - accuracy: 0.0583\n",
      "Epoch 1: saving model to training_1/cp.ckpt\n",
      "21/21 [==============================] - 26s 1s/step - loss: 0.3723 - accuracy: 0.0583 - val_loss: 0.1944 - val_accuracy: 0.0493\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1871 - accuracy: 0.1166\n",
      "Epoch 2: saving model to training_1/cp.ckpt\n",
      "21/21 [==============================] - 23s 1s/step - loss: 0.1871 - accuracy: 0.1166 - val_loss: 0.1812 - val_accuracy: 0.0493\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1710 - accuracy: 0.2255\n",
      "Epoch 3: saving model to training_1/cp.ckpt\n",
      "21/21 [==============================] - 23s 1s/step - loss: 0.1710 - accuracy: 0.2255 - val_loss: 0.1675 - val_accuracy: 0.1761\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1516 - accuracy: 0.3252\n",
      "Epoch 4: saving model to training_1/cp.ckpt\n",
      "21/21 [==============================] - 23s 1s/step - loss: 0.1516 - accuracy: 0.3252 - val_loss: 0.1485 - val_accuracy: 0.1690\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1299 - accuracy: 0.3466\n",
      "Epoch 5: saving model to training_1/cp.ckpt\n",
      "21/21 [==============================] - 23s 1s/step - loss: 0.1299 - accuracy: 0.3466 - val_loss: 0.1335 - val_accuracy: 0.2394\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1143 - accuracy: 0.4095\n",
      "Epoch 6: saving model to training_1/cp.ckpt\n",
      "21/21 [==============================] - 23s 1s/step - loss: 0.1143 - accuracy: 0.4095 - val_loss: 0.1246 - val_accuracy: 0.3451\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1034 - accuracy: 0.4755\n",
      "Epoch 7: saving model to training_1/cp.ckpt\n",
      "21/21 [==============================] - 23s 1s/step - loss: 0.1034 - accuracy: 0.4755 - val_loss: 0.1201 - val_accuracy: 0.3592\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0947 - accuracy: 0.5491\n",
      "Epoch 8: saving model to training_1/cp.ckpt\n",
      "21/21 [==============================] - 23s 1s/step - loss: 0.0947 - accuracy: 0.5491 - val_loss: 0.1078 - val_accuracy: 0.5211\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0854 - accuracy: 0.5598\n",
      "Epoch 9: saving model to training_1/cp.ckpt\n",
      "21/21 [==============================] - 23s 1s/step - loss: 0.0854 - accuracy: 0.5598 - val_loss: 0.1008 - val_accuracy: 0.5070\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.6058\n",
      "Epoch 10: saving model to training_1/cp.ckpt\n",
      "21/21 [==============================] - 23s 1s/step - loss: 0.0780 - accuracy: 0.6058 - val_loss: 0.0956 - val_accuracy: 0.5563\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.6166\n",
      "Epoch 11: saving model to training_1/cp.ckpt\n",
      "21/21 [==============================] - 23s 1s/step - loss: 0.0706 - accuracy: 0.6166 - val_loss: 0.0899 - val_accuracy: 0.5915\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.6411\n",
      "Epoch 12: saving model to training_1/cp.ckpt\n",
      "21/21 [==============================] - 23s 1s/step - loss: 0.0649 - accuracy: 0.6411 - val_loss: 0.0908 - val_accuracy: 0.5915\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0599 - accuracy: 0.6488\n",
      "Epoch 13: saving model to training_1/cp.ckpt\n",
      "21/21 [==============================] - 23s 1s/step - loss: 0.0599 - accuracy: 0.6488 - val_loss: 0.0842 - val_accuracy: 0.6268\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.6580\n",
      "Epoch 14: saving model to training_1/cp.ckpt\n",
      "21/21 [==============================] - 23s 1s/step - loss: 0.0550 - accuracy: 0.6580 - val_loss: 0.0873 - val_accuracy: 0.5915\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0499 - accuracy: 0.6810\n",
      "Epoch 15: saving model to training_1/cp.ckpt\n",
      "21/21 [==============================] - 23s 1s/step - loss: 0.0499 - accuracy: 0.6810 - val_loss: 0.0838 - val_accuracy: 0.6127\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.6779\n",
      "Epoch 16: saving model to training_1/cp.ckpt\n",
      "21/21 [==============================] - 23s 1s/step - loss: 0.0474 - accuracy: 0.6779 - val_loss: 0.0836 - val_accuracy: 0.6127\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0446 - accuracy: 0.6810\n",
      "Epoch 17: saving model to training_1/cp.ckpt\n",
      "21/21 [==============================] - 23s 1s/step - loss: 0.0446 - accuracy: 0.6810 - val_loss: 0.0863 - val_accuracy: 0.6338\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.6948\n",
      "Epoch 18: saving model to training_1/cp.ckpt\n",
      "21/21 [==============================] - 23s 1s/step - loss: 0.0408 - accuracy: 0.6948 - val_loss: 0.0903 - val_accuracy: 0.6479\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.6994\n",
      "Epoch 19: saving model to training_1/cp.ckpt\n",
      "21/21 [==============================] - 24s 1s/step - loss: 0.0385 - accuracy: 0.6994 - val_loss: 0.0924 - val_accuracy: 0.6408\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.7117\n",
      "Epoch 20: saving model to training_1/cp.ckpt\n",
      "21/21 [==============================] - 24s 1s/step - loss: 0.0349 - accuracy: 0.7117 - val_loss: 0.0903 - val_accuracy: 0.6268\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.7132\n",
      "Epoch 21: saving model to training_1/cp.ckpt\n",
      "21/21 [==============================] - 24s 1s/step - loss: 0.0351 - accuracy: 0.7132 - val_loss: 0.0892 - val_accuracy: 0.6479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe8b9f1fdf0>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 250\n",
    "input = tf.keras.layers.Input(shape=(MAX_LEN,))\n",
    "x = tf.keras.layers.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(input)\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(256, return_sequences=True, dropout=0.1,\n",
    "                                                      recurrent_dropout=0.1))(x)\n",
    "x = tf.keras.layers.Conv1D(128, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(x)\n",
    "avg_pool = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "max_pool = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "x = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
    "\n",
    "preds = tf.keras.layers.Dense(y_train.shape[1], activation=\"sigmoid\")(x)\n",
    "\n",
    "model = tf.keras.Model(input, preds)\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-3)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    cp_callback\n",
    "]\n",
    "# model.fit(X_train, y_train, verbose=1, epochs=10, validation_split=0.2, callbacks=callbacks)\n",
    "model.fit(X_train_data, y_train_data, verbose=1, epochs=50, validation_data=(X_test_data, y_test_data), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 4s 165ms/step - loss: 0.0409 - accuracy: 0.7166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.040889158844947815, 0.7166246771812439]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.000'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{:.3f}'.format(3.6592403e-13)\n",
    "#  9.9997795e-01 -> 0.99997795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tf_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tf_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('tf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('tf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connect vpn\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "Tags tidak ditemukan\n"
     ]
    }
   ],
   "source": [
    "faq = 'cara connect vpn'\n",
    "faq = preprocessingText(faq)\n",
    "print(faq)\n",
    "tokenize_faq = word_tokenize(faq)\n",
    "tokenize_faq = [val for val in tokenize_faq if val not in stop_words]\n",
    "faq = \" \".join(tokenize_faq)\n",
    "new_faq = [faq]\n",
    "seq = tokenizer.texts_to_sequences(new_faq)\n",
    "padded = tf.keras.utils.pad_sequences(seq, MAX_SEQUENCE_LENGTH)\n",
    "pred = model.predict(padded)\n",
    "# print(pred)\n",
    "threshold = 0.5\n",
    "found = False\n",
    "for i in range(len(selected_column)):\n",
    "    if pred[0][i] > threshold:\n",
    "        found = True\n",
    "        pred1 = '{:.3f}'.format(pred[0][i])\n",
    "        print(selected_column[i], pred1)\n",
    "\n",
    "if not found:\n",
    "    print(\"Tags tidak ditemukan\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
